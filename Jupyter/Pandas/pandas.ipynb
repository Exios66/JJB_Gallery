{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba7436ce",
   "metadata": {},
   "source": [
    "# Pandas Data Workflow: End-to-End Guide\n",
    "\n",
    "This notebook demonstrates how to use core and advanced pandas functionality for a full analytics workflow: data ingestion, exploration, cleaning, transformation, feature engineering, and preparing design matrices suitable for scikit-learn model training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44d8894",
   "metadata": {},
   "source": [
    "## Workflow Roadmap\n",
    "\n",
    "1. Environment setup and consistent plotting/theme configuration\n",
    "2. Data ingestion from CSV (penguins dataset) and initial inspection\n",
    "3. Core pandas operations: indexing, selection, filtering, sorting\n",
    "4. Exploratory data analysis (EDA): descriptive statistics, grouping, aggregation, reshaping, visualization hooks\n",
    "5. Data quality: missing values, outliers, duplicates, type casting\n",
    "6. Feature engineering: dates, categorical encoding, numerical transformations\n",
    "7. Preparing train-ready feature (`X`) and target (`y`) matrices, including scikit-learn compatible pipelines\n",
    "8. Lightweight model training example to validate the dataset hand-off to scikit-learn\n",
    "9. Saving artifacts and documenting reusable utilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b11c6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "sns.set_theme(style=\"whitegrid\", context=\"talk\")\n",
    "\n",
    "REPO_ROOT = pathlib.Path(\"../../..\").resolve()\n",
    "RAW_DATA_URL = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e078b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import subprocess\n",
    "\n",
    "def ensure(package: str):\n",
    "    try:\n",
    "        importlib.import_module(package)\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "for pkg in (\"pyarrow\",):\n",
    "    ensure(pkg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6902bb75",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion & Initial Inspection\n",
    "\n",
    "We use the penguins dataset, emphasizing how pandas reads remote CSV data, persists raw copies locally when needed, and inspects dtypes/shape/metadata.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299aad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = REPO_ROOT / \"data\" / \"penguins_raw.csv\"\n",
    "raw_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_raw = pd.read_csv(RAW_DATA_URL)\n",
    "df_raw.to_csv(raw_path, index=False)\n",
    "\n",
    "print(f\"Rows: {df_raw.shape[0]}, Columns: {df_raw.shape[1]}\")\n",
    "df_raw.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc80907",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4593faba",
   "metadata": {},
   "source": [
    "## 2. Core Pandas Operations\n",
    "\n",
    "Below we demonstrate slicing, boolean masks, sorting, selecting columns, and leveraging `assign`, `pipe`, and `query` for expressive transformations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fe3103",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = [\"species\", \"island\", \"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\", \"sex\"]\n",
    "df = df_raw[selected_cols].copy()\n",
    "\n",
    "adelie_mask = df[\"species\"].eq(\"Adelie\")\n",
    "adelie_sorted = df.loc[adelie_mask].sort_values(by=\"bill_length_mm\", ascending=False)\n",
    "adelie_sorted.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072d9d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bill_ratio(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    return frame.assign(bill_ratio=frame[\"bill_length_mm\"] / frame[\"bill_depth_mm\"])\n",
    "\n",
    "(df\n",
    " .dropna(subset=[\"bill_length_mm\", \"bill_depth_mm\"])\n",
    " .pipe(calc_bill_ratio)\n",
    " .query(\"bill_ratio > 2.5\")\n",
    " .head()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19137bb0",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Use `describe`, `value_counts`, `groupby`, and visualization-ready summaries for deeper understanding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194d353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=\"all\").T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c744849",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = (df\n",
    "           .dropna(subset=[\"body_mass_g\"])\n",
    "           .groupby([\"species\", \"sex\"], observed=True)\n",
    "           .agg(\n",
    "               mean_mass=(\"body_mass_g\", \"mean\"),\n",
    "               sd_mass=(\"body_mass_g\", \"std\"),\n",
    "               n=(\"body_mass_g\", \"size\")\n",
    "           )\n",
    "           .reset_index()\n",
    "          )\n",
    "grouped\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c8b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = pd.pivot_table(\n",
    "    grouped,\n",
    "    values=\"mean_mass\",\n",
    "    index=\"species\",\n",
    "    columns=\"sex\",\n",
    "    aggfunc=\"mean\"\n",
    ")\n",
    "pivot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f17fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(data=df, x=\"body_mass_g\", hue=\"species\", kde=True)\n",
    "plt.title(\"Body Mass Distribution by Species\")\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf338090",
   "metadata": {},
   "source": [
    "## 4. Data Quality & Cleaning\n",
    "\n",
    "Identify missingness, duplicate handling, outlier rules, and dtype normalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcedf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_profile = df.isna().mean().mul(100).round(2)\n",
    "missing_profile\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2cf36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df.duplicated(subset=[\"species\", \"island\", \"bill_length_mm\", \"bill_depth_mm\", \"body_mass_g\"])\n",
    "duplicates.sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb9bd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = df.drop_duplicates().copy()\n",
    "\n",
    "numeric_cols = clean_df.select_dtypes(include=\"number\").columns\n",
    "clean_df[numeric_cols] = clean_df[numeric_cols].apply(lambda col: col.fillna(col.median()))\n",
    "\n",
    "clean_df[\"sex\"] = clean_df[\"sex\"].fillna(\"Unknown\").astype(\"category\")\n",
    "clean_df[\"species\"] = clean_df[\"species\"].astype(\"category\")\n",
    "clean_df.dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114707e1",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n",
    "\n",
    "Demonstrate numerical scaling, binning, categorical encoding, interaction features, and tidy reshaping patterns.\n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c297908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = (clean_df\n",
    "              .assign(\n",
    "                  body_mass_kg=lambda d: d[\"body_mass_g\"] / 1000,\n",
    "                  flipper_to_bill=lambda d: d[\"flipper_length_mm\"] / d[\"bill_length_mm\"],\n",
    "                  mass_bin=lambda d: pd.cut(d[\"body_mass_g\"], bins=[2500, 4000, 5000, 6500], labels=[\"small\", \"medium\", \"large\"])\n",
    "              )\n",
    "             )\n",
    "feature_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40bcf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df = feature_df.melt(\n",
    "    id_vars=[\"species\", \"island\", \"sex\"],\n",
    "    value_vars=[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"],\n",
    "    var_name=\"measurement\",\n",
    "    value_name=\"value\"\n",
    ")\n",
    "long_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a251e4",
   "metadata": {},
   "source": [
    "## 6. Train/Test Split & Modeling Prep\n",
    "\n",
    "Define `X` (features) and `y` (labels), encode categoricals, scale numerics, and build a scikit-learn pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04dea90",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"species\"\n",
    "FEATURES = [col for col in feature_df.columns if col not in {TARGET}]\n",
    "\n",
    "X = feature_df[FEATURES].copy()\n",
    "y = feature_df[TARGET].copy()\n",
    "\n",
    "numeric_features = X.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=[\"number\"]).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704705fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", LogisticRegression(max_iter=200))\n",
    "])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefea9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_feature_names = clf.named_steps[\"preprocess\"].named_transformers_[\"cat\"].named_steps[\"onehot\"].get_feature_names_out(categorical_features)\n",
    "print(\"Numeric features:\", numeric_features)\n",
    "print(\"Encoded categorical features:\", list(ohe_feature_names)[:5], \"...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524e0c29",
   "metadata": {},
   "source": [
    "## 7. Persisting Processed Data\n",
    "\n",
    "Save cleaned data and modeling splits for reproducibility.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ecb2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dir = REPO_ROOT / \"data\" / \"processed\"\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "feature_df.to_parquet(processed_dir / \"penguins_features.parquet\", index=False)\n",
    "X_train.to_parquet(processed_dir / \"penguins_X_train.parquet\", index=False)\n",
    "X_test.to_parquet(processed_dir / \"penguins_X_test.parquet\", index=False)\n",
    "y_train.to_frame(name=TARGET).to_csv(processed_dir / \"penguins_y_train.csv\", index=False)\n",
    "y_test.to_frame(name=TARGET).to_csv(processed_dir / \"penguins_y_test.csv\", index=False)\n",
    "\n",
    "processed_dir\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994f9ef6",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- Pandas simplifies the entire analytics lifecycle: ingest, explore, clean, engineer, and persist datasets.\n",
    "- Consistent schemas and dtype management ensure painless hand-offs to scikit-learn pipelines.\n",
    "- Saving processed data and reusing transformation utilities preserves reproducibility and accelerates iteration.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
