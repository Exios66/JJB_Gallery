[
  {
    "objectID": "Quarto/randomforest.html",
    "href": "Quarto/randomforest.html",
    "title": "Random Forests in Practice",
    "section": "",
    "text": "Random forests are ensemble models that aggregate many decision trees to reduce variance and improve generalization.1 This document walks through training and interpreting a random forest classifier in Python, with a mix of narrative, math, and visuals."
  },
  {
    "objectID": "Quarto/randomforest.html#overview",
    "href": "Quarto/randomforest.html#overview",
    "title": "Random Forests in Practice",
    "section": "",
    "text": "Random forests are ensemble models that aggregate many decision trees to reduce variance and improve generalization.1 This document walks through training and interpreting a random forest classifier in Python, with a mix of narrative, math, and visuals."
  },
  {
    "objectID": "Quarto/randomforest.html#mathematical-model",
    "href": "Quarto/randomforest.html#mathematical-model",
    "title": "Random Forests in Practice",
    "section": "Mathematical Model",
    "text": "Mathematical Model\nEach tree \\(T_b\\) is trained on a bootstrap sample \\(\\mathcal{D}_b\\) and a random subset of features. The forest prediction for a classification task with \\(B\\) trees is the majority vote:\n\\[\n\\hat{y} = \\mathrm{mode}\\left(\\{T_b(\\mathbf{x})\\}_{b=1}^{B}\\right)\n\\]\nFor regression, the trees are averaged:\n\\[\n\\hat{y} = \\frac{1}{B}\\sum_{b=1}^{B} T_b(\\mathbf{x})\n\\]\nThe randomization across bootstrapped data and feature subsampling drives decorrelation between trees, delivering lower variance than single-tree models."
  },
  {
    "objectID": "Quarto/randomforest.html#environment-setup",
    "href": "Quarto/randomforest.html#environment-setup",
    "title": "Random Forests in Practice",
    "section": "Environment Setup",
    "text": "Environment Setup\n\n\nCode\nimport importlib\nimport subprocess\nimport sys\n\ndef ensure(package):\n    try:\n        importlib.import_module(package)\n    except ImportError:\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n\nfor pkg in (\"numpy\", \"pandas\", \"seaborn\", \"matplotlib\", \"scikit-learn\"):\n    ensure(pkg)\n\n\nRequirement already satisfied: scikit-learn in /Users/luciusjmorningstar/Desktop/GIT-REPOSITORY/JJB_Gallery/.venv/lib/python3.13/site-packages (1.7.2)\nRequirement already satisfied: numpy&gt;=1.22.0 in /Users/luciusjmorningstar/Desktop/GIT-REPOSITORY/JJB_Gallery/.venv/lib/python3.13/site-packages (from scikit-learn) (2.3.5)\nRequirement already satisfied: scipy&gt;=1.8.0 in /Users/luciusjmorningstar/Desktop/GIT-REPOSITORY/JJB_Gallery/.venv/lib/python3.13/site-packages (from scikit-learn) (1.16.3)\nRequirement already satisfied: joblib&gt;=1.2.0 in /Users/luciusjmorningstar/Desktop/GIT-REPOSITORY/JJB_Gallery/.venv/lib/python3.13/site-packages (from scikit-learn) (1.5.2)\nRequirement already satisfied: threadpoolctl&gt;=3.1.0 in /Users/luciusjmorningstar/Desktop/GIT-REPOSITORY/JJB_Gallery/.venv/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_breast_cancer, make_classification\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import RocCurveDisplay, ConfusionMatrixDisplay, classification_report\nfrom sklearn.decomposition import PCA\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nsns.set_theme(style=\"whitegrid\")"
  },
  {
    "objectID": "Quarto/randomforest.html#data-loading-and-preparation",
    "href": "Quarto/randomforest.html#data-loading-and-preparation",
    "title": "Random Forests in Practice",
    "section": "Data Loading and Preparation",
    "text": "Data Loading and Preparation\nWe will use the Breast Cancer Wisconsin dataset bundled with scikit-learn, which contains 30 features computed from digitized fine needle aspirate images.2\n\n\nCode\ndataset = load_breast_cancer(as_frame=True)\ndf = dataset.frame\ndf.head()\n\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\ntarget\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n...\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n0\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n...\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n0\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n...\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n0\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n0.09744\n...\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n0\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n0.05883\n...\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n0\n\n\n\n\n5 rows √ó 31 columns\n\n\n\nSplit the data into training and testing sets (stratified to maintain label balance).\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(\n    df.drop(columns=\"target\"),\n    df[\"target\"],\n    test_size=0.25,\n    random_state=42,\n    stratify=df[\"target\"]\n)\n\nX_train.shape, X_test.shape\n\n\n((426, 30), (143, 30))"
  },
  {
    "objectID": "Quarto/randomforest.html#model-training",
    "href": "Quarto/randomforest.html#model-training",
    "title": "Random Forests in Practice",
    "section": "Model Training",
    "text": "Model Training\n\n\nCode\nrf = RandomForestClassifier(\n    n_estimators=400,\n    max_features=\"sqrt\",\n    min_samples_leaf=2,\n    random_state=42,\n    n_jobs=-1\n)\nrf.fit(X_train, y_train)\n\n\nRandomForestClassifier(min_samples_leaf=2, n_estimators=400, n_jobs=-1,\n                       random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifier?Documentation for RandomForestClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_estimators¬†\n400\n\n\n\ncriterion¬†\n'gini'\n\n\n\nmax_depth¬†\nNone\n\n\n\nmin_samples_split¬†\n2\n\n\n\nmin_samples_leaf¬†\n2\n\n\n\nmin_weight_fraction_leaf¬†\n0.0\n\n\n\nmax_features¬†\n'sqrt'\n\n\n\nmax_leaf_nodes¬†\nNone\n\n\n\nmin_impurity_decrease¬†\n0.0\n\n\n\nbootstrap¬†\nTrue\n\n\n\noob_score¬†\nFalse\n\n\n\nn_jobs¬†\n-1\n\n\n\nrandom_state¬†\n42\n\n\n\nverbose¬†\n0\n\n\n\nwarm_start¬†\nFalse\n\n\n\nclass_weight¬†\nNone\n\n\n\nccp_alpha¬†\n0.0\n\n\n\nmax_samples¬†\nNone\n\n\n\nmonotonic_cst¬†\nNone\n\n\n\n\n            \n        \n    \n\n\nEvaluate cross-validated training performance to estimate generalization ability.\n\n\nCode\ncv_scores = cross_val_score(rf, X_train, y_train, cv=5)\ncv_scores.mean(), cv_scores.std()\n\n\n(np.float64(0.9600547195622434), np.float64(0.020556647327639923))"
  },
  {
    "objectID": "Quarto/randomforest.html#diagnostics",
    "href": "Quarto/randomforest.html#diagnostics",
    "title": "Random Forests in Practice",
    "section": "Diagnostics",
    "text": "Diagnostics\n\n\nCode\ny_pred = rf.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=dataset.target_names))\n\n\n              precision    recall  f1-score   support\n\n   malignant       0.96      0.92      0.94        53\n      benign       0.96      0.98      0.97        90\n\n    accuracy                           0.96       143\n   macro avg       0.96      0.95      0.95       143\nweighted avg       0.96      0.96      0.96       143\n\n\n\n\nReceiver Operating Characteristic\n\n\nCode\nfig, ax = plt.subplots(figsize=(8, 6))\nRocCurveDisplay.from_estimator(rf, X_test, y_test, ax=ax)\nax.set_title(\"Random Forest ROC Curve\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nConfusion Matrix\n\n\nCode\nfig, ax = plt.subplots(figsize=(6, 6))\nConfusionMatrixDisplay.from_estimator(rf, X_test, y_test, display_labels=dataset.target_names, ax=ax, cmap=\"Blues\")\nax.set_title(\"Confusion Matrix\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Quarto/randomforest.html#feature-importance-visualization",
    "href": "Quarto/randomforest.html#feature-importance-visualization",
    "title": "Random Forests in Practice",
    "section": "Feature Importance Visualization",
    "text": "Feature Importance Visualization\n\n\nCode\nimportances = pd.Series(rf.feature_importances_, index=df.columns[:-1]).sort_values(ascending=False)\ntop_features = importances.head(15)\n\nplt.figure(figsize=(10, 8))\nsns.barplot(x=top_features.values, y=top_features.index, palette=\"viridis\")\nplt.title(\"Top 15 Feature Importances (Gini)\")\nplt.xlabel(\"Importance\")\nplt.ylabel(\"Feature\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Quarto/randomforest.html#complex-visualization-decision-boundaries",
    "href": "Quarto/randomforest.html#complex-visualization-decision-boundaries",
    "title": "Random Forests in Practice",
    "section": "Complex Visualization: Decision Boundaries",
    "text": "Complex Visualization: Decision Boundaries\nTo visualize the decision boundaries of the high-dimensional Random Forest model, we project the data onto its first two Principal Components (PCA). This allows us to see how the model separates classes in a 2D latent space.\n\n\nCode\n# PCA Projection\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_train)\n\n# Train a new RF on PCA components for visualization\nrf_pca = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_pca.fit(X_pca, y_train)\n\n# Plot Decision Boundary\nfig, ax = plt.subplots(figsize=(10, 8))\nDecisionBoundaryDisplay.from_estimator(\n    rf_pca,\n    X_pca,\n    response_method=\"predict\",\n    cmap=\"RdBu\",\n    alpha=0.8,\n    ax=ax,\n    xlabel=\"Principal Component 1\",\n    ylabel=\"Principal Component 2\",\n)\nscatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=y_train, cmap=\"RdBu\", edgecolors=\"k\", s=30)\nax.set_title(\"Random Forest Decision Boundaries (PCA Projected)\")\nplt.legend(*scatter.legend_elements(), title=\"Classes\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Quarto/randomforest.html#error-rate-vs.-number-of-trees",
    "href": "Quarto/randomforest.html#error-rate-vs.-number-of-trees",
    "title": "Random Forests in Practice",
    "section": "Error Rate vs.¬†Number of Trees",
    "text": "Error Rate vs.¬†Number of Trees\nThis graph illustrates how the model‚Äôs Out-of-Bag (OOB) error rate stabilizes as the number of trees in the forest increases, demonstrating the ensemble effect.\n\n\nCode\nn_estimators_range = range(15, 300, 10)\noob_errors = []\n\nfor n in n_estimators_range:\n    rf_oob = RandomForestClassifier(n_estimators=n, warm_start=True, oob_score=True, random_state=42, n_jobs=-1)\n    rf_oob.fit(X_train, y_train)\n    oob_errors.append(1 - rf_oob.oob_score_)\n\nplt.figure(figsize=(10, 6))\nplt.plot(n_estimators_range, oob_errors, marker='o', linestyle='-', color='purple')\nplt.title(\"OOB Error Rate vs. Number of Trees\")\nplt.xlabel(\"Number of Trees (n_estimators)\")\nplt.ylabel(\"OOB Error Rate\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Quarto/randomforest.html#hyperparameter-considerations",
    "href": "Quarto/randomforest.html#hyperparameter-considerations",
    "title": "Random Forests in Practice",
    "section": "Hyperparameter Considerations",
    "text": "Hyperparameter Considerations\n\nn_estimators: Increasing trees generally improves stability until diminishing returns set in.\nmax_depth or min_samples_leaf: Control tree complexity, mitigating overfitting.\nmax_features: Governs the degree of feature randomness; sqrt is typical for classification.\nclass_weight: Useful for imbalanced datasets to penalize misclassification of minority classes.\n\nGrid search or Bayesian optimization can systematically explore these settings.3"
  },
  {
    "objectID": "Quarto/randomforest.html#practical-tips",
    "href": "Quarto/randomforest.html#practical-tips",
    "title": "Random Forests in Practice",
    "section": "Practical Tips",
    "text": "Practical Tips\n\nFeature scaling: Not required because trees are invariant to monotonic transformations.\nMissing values: scikit-learn‚Äôs implementation does not handle NaNs; impute beforehand.\nInterpretability: Use SHAP values or permutation importance for richer explanations.\nOut-of-bag (OOB) estimates: Enable oob_score=True to get a built-in validation metric without a separate hold-out set."
  },
  {
    "objectID": "Quarto/randomforest.html#references",
    "href": "Quarto/randomforest.html#references",
    "title": "Random Forests in Practice",
    "section": "References",
    "text": "References\n\nBreiman, L. (2001). Random forests. Machine Learning, 45(1), 5‚Äì32. https://doi.org/10.1023/A:10109334043244\nscikit-learn Breast Cancer Dataset docs. https://scikit-learn.org/stable/datasets/toy_dataset.html5\nBergstra, J., & Bengio, Y. (2012). Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13, 281‚Äì305. https://jmlr.org/papers/v13/bergstra12a.html6"
  },
  {
    "objectID": "Quarto/randomforest.html#footnotes",
    "href": "Quarto/randomforest.html#footnotes",
    "title": "Random Forests in Practice",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIntroduced the random forest algorithm with theoretical justification and empirical benchmarks.‚Ü©Ô∏é\nOfficial description of the dataset, feature definitions, and usage considerations.‚Ü©Ô∏é\nDemonstrated the efficiency gains of random search over grid search for hyperparameter tuning.‚Ü©Ô∏é\n^breiman2001‚Ü©Ô∏é\n^sklearn_breast‚Ü©Ô∏é\n^bergstra2012‚Ü©Ô∏é"
  },
  {
    "objectID": "projects/Crewai/README.html",
    "href": "projects/Crewai/README.html",
    "title": "CrewAI Multi-Agent Swarm System",
    "section": "",
    "text": "A comprehensive multi-agent framework using CrewAI, featuring multiple specialized agent swarms for different domains including machine learning, research, development, business intelligence, and documentation.\n\n\nThis project implements a versatile multi-agent system where different specialized agent swarms collaborate on complex workflows. Each swarm contains domain-specific agents designed for particular tasks and industries.\n\n\n\n\n\nInteract with the swarms using a modern chat interface.\nstreamlit run interface_web.py\n\n\n\nInteract via the command line.\npython interface_cli.py\n\n\n\n\n\n\nSpecialization: Random Forest and machine learning evaluation Agents: 5 specialized ML agents\n\nData Analyst: Dataset exploration and preprocessing recommendations\nModel Evaluator: Performance assessment and comparison analysis\nFeature Engineer: Feature importance analysis and engineering suggestions\nHyperparameter Optimizer: Parameter tuning and optimization strategies\nReport Writer: Comprehensive report generation for stakeholders\n\n\n\n\nSpecialization: ML research, trends, and innovation Agents: 4 research-focused agents\n\nLiterature Reviewer: Academic paper analysis and scholarly research\nTrend Analyzer: Industry trends and market developments\nInnovation Scout: Novel applications and breakthrough technologies\nResearch Summarizer: Synthesis of research findings\n\n\n\n\nSpecialization: Scholarly research and academic analysis Agents: 5 academic research agents\n\nLiterature Reviewer: Comprehensive academic literature review\nResearch Designer: Experimental design and methodology\nAcademic Data Analyst: Statistical analysis for research"
  },
  {
    "objectID": "projects/Crewai/README.html#overview",
    "href": "projects/Crewai/README.html#overview",
    "title": "CrewAI Multi-Agent Swarm System",
    "section": "",
    "text": "This project implements a versatile multi-agent system where different specialized agent swarms collaborate on complex workflows. Each swarm contains domain-specific agents designed for particular tasks and industries."
  },
  {
    "objectID": "projects/Crewai/README.html#chat-interfaces",
    "href": "projects/Crewai/README.html#chat-interfaces",
    "title": "CrewAI Multi-Agent Swarm System",
    "section": "",
    "text": "Interact with the swarms using a modern chat interface.\nstreamlit run interface_web.py\n\n\n\nInteract via the command line.\npython interface_cli.py"
  },
  {
    "objectID": "projects/Crewai/README.html#available-agent-swarms",
    "href": "projects/Crewai/README.html#available-agent-swarms",
    "title": "CrewAI Multi-Agent Swarm System",
    "section": "",
    "text": "Specialization: Random Forest and machine learning evaluation Agents: 5 specialized ML agents\n\nData Analyst: Dataset exploration and preprocessing recommendations\nModel Evaluator: Performance assessment and comparison analysis\nFeature Engineer: Feature importance analysis and engineering suggestions\nHyperparameter Optimizer: Parameter tuning and optimization strategies\nReport Writer: Comprehensive report generation for stakeholders\n\n\n\n\nSpecialization: ML research, trends, and innovation Agents: 4 research-focused agents\n\nLiterature Reviewer: Academic paper analysis and scholarly research\nTrend Analyzer: Industry trends and market developments\nInnovation Scout: Novel applications and breakthrough technologies\nResearch Summarizer: Synthesis of research findings\n\n\n\n\nSpecialization: Scholarly research and academic analysis Agents: 5 academic research agents\n\nLiterature Reviewer: Comprehensive academic literature review\nResearch Designer: Experimental design and methodology\nAcademic Data Analyst: Statistical analysis for research"
  },
  {
    "objectID": "CHANGELOG.html",
    "href": "CHANGELOG.html",
    "title": "CHANGELOG",
    "section": "",
    "text": "Resolved Git repository corruption issues: Fixed corrupted remote references and missing object errors\nImproved repository organization: Moved all macOS resource fork files (._* files) to Xtra_Copies directory\n\n\n\n\n\nRepository corruption: Resolved issues with corrupted refs/remotes/origin/gh-pages and missing Git objects\nFile organization: Cleaned up macOS metadata files by consolidating them in Xtra_Copies\nDependency Management: Resolved Python environment corruption caused by disk space exhaustion; recreated virtual environment and reinstalled all dependencies\nDocumentation: Fixed Table of Contents overlay issue in index.html by updating Quarto configuration\n\n\n\n\n\nUpdated .gitignore to ignore macOS resource fork files (._*) and .DS_Store files\nRemoved all tracked ._* files from Git repository\n\n\n\n\n\n\n\n\nAdded project subdirectory\nAdded Jupyter notebooks of both pandas and SciKit Learn essentials.\nAdded a Quarto document on the essentials underlying the random forest algorithm.\nAdded a Quarto document on the random forest algorithm in practice.\nAdded a script to render the random forest document to a html and pdf file.\nAdded a script to start a preview server for the random forest document.\nEnhanced documentation and structure: Added Quarto document on random forest application, updated CHANGELOG, modified index.html for improved responsiveness, and expanded CrewAI project README with detailed agent swarm descriptions.\n\n\n\n\n\n\n\n\n\n\nAdded requirements.txt with the following dependencies:\n\nnumpy\npandas\nseaborn\nmatplotlib\nscikit-learn\n\nAdded .gitignore\nImproved README documentation\n\n\n\n\n\n\nN/A\n\n\n\n\n\nN/A"
  },
  {
    "objectID": "CHANGELOG.html#section",
    "href": "CHANGELOG.html#section",
    "title": "CHANGELOG",
    "section": "",
    "text": "Resolved Git repository corruption issues: Fixed corrupted remote references and missing object errors\nImproved repository organization: Moved all macOS resource fork files (._* files) to Xtra_Copies directory\n\n\n\n\n\nRepository corruption: Resolved issues with corrupted refs/remotes/origin/gh-pages and missing Git objects\nFile organization: Cleaned up macOS metadata files by consolidating them in Xtra_Copies\nDependency Management: Resolved Python environment corruption caused by disk space exhaustion; recreated virtual environment and reinstalled all dependencies\nDocumentation: Fixed Table of Contents overlay issue in index.html by updating Quarto configuration\n\n\n\n\n\nUpdated .gitignore to ignore macOS resource fork files (._*) and .DS_Store files\nRemoved all tracked ._* files from Git repository"
  },
  {
    "objectID": "CHANGELOG.html#section-1",
    "href": "CHANGELOG.html#section-1",
    "title": "CHANGELOG",
    "section": "",
    "text": "Added project subdirectory\nAdded Jupyter notebooks of both pandas and SciKit Learn essentials.\nAdded a Quarto document on the essentials underlying the random forest algorithm.\nAdded a Quarto document on the random forest algorithm in practice.\nAdded a script to render the random forest document to a html and pdf file.\nAdded a script to start a preview server for the random forest document.\nEnhanced documentation and structure: Added Quarto document on random forest application, updated CHANGELOG, modified index.html for improved responsiveness, and expanded CrewAI project README with detailed agent swarm descriptions."
  },
  {
    "objectID": "CHANGELOG.html#section-2",
    "href": "CHANGELOG.html#section-2",
    "title": "CHANGELOG",
    "section": "",
    "text": "Added requirements.txt with the following dependencies:\n\nnumpy\npandas\nseaborn\nmatplotlib\nscikit-learn\n\nAdded .gitignore\nImproved README documentation\n\n\n\n\n\n\nN/A\n\n\n\n\n\nN/A"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GitHub-Based Portfolio & Gallery of Jack J. Burleson",
    "section": "",
    "text": "Welcome! This repository serves as the official portfolio and gallery for Jack J. Burleson, showcasing a curated selection of previous work, open-source projects, research, and presentations."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "GitHub-Based Portfolio & Gallery of Jack J. Burleson",
    "section": "About Me",
    "text": "About Me\nHi! I‚Äôm Jack J. Burleson ‚Äì data scientist, research engineer, and open-source enthusiast.\nI am passionate about making data science, machine learning, and advanced analytics accessible and meaningful through clear code and insightful visualizations.\nThis portfolio highlights select projects in engineering, data analysis, machine learning, and technical writing."
  },
  {
    "objectID": "index.html#project-gallery",
    "href": "index.html#project-gallery",
    "title": "GitHub-Based Portfolio & Gallery of Jack J. Burleson",
    "section": "Project Gallery",
    "text": "Project Gallery\n\n\n\nProject\nDescription\nLink\n\n\n\n\nRandom Forest Essentials\n[New] Comprehensive analysis with interactive plots, decision boundaries, and ROC curves.\nüìä View Interactive Analysis\n\n\nCrewAI Multi-Agent Swarm System\nMulti-agent architecture using CrewAI, with specialized swarms for ML, research, business intelligence, and documentation; includes Streamlit & CLI interfaces.\nüöÄ Launch Swarm | üìÑ Docs\n\n\nTerminal Agents\nAI coding agents for the terminal; includes ‚Äúbuild‚Äù and ‚Äúplan‚Äù agents, supporting code exploration and editing with easy installation and cross-platform support.\nterminal_agents/README.md\n\n\nJupyter ML & Pandas Notebooks\nCollections of essential notebooks demonstrating the use of Pandas and scikit-learn in data workflows.\nnotebooks/\n\n\nPyPI-Ready Python Template\nModern Python project template with pre-commit, Black, and CI configuration, ready for immediate use as a best-practices starter.\nTemplate Repo\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\nMore projects, research, and documentation are regularly added. See repository folders for latest content."
  },
  {
    "objectID": "index.html#interactive-agent-swarm",
    "href": "index.html#interactive-agent-swarm",
    "title": "GitHub-Based Portfolio & Gallery of Jack J. Burleson",
    "section": "Interactive Agent Swarm",
    "text": "Interactive Agent Swarm\n\n\n\n\n\n\nTipü§ñ Run the Live Interface\n\n\n\nThis portfolio contains a full CrewAI Multi-Agent System application.\nTo interact with the specialized agent swarms (ML, Research, Business Intelligence), you can run the Streamlit interface locally.\n\n\n\nüöÄ Quick Start\n\nClone this repository to your local machine.\nInstall dependencies: bash     pip install -r requirements.txt\nLaunch the Swarm Interface: bash     streamlit run projects/Crewai/interface_web.py\n\nüìÇ Browse Source Code | üìÑ View Documentation"
  },
  {
    "objectID": "index.html#recent-additions",
    "href": "index.html#recent-additions",
    "title": "GitHub-Based Portfolio & Gallery of Jack J. Burleson",
    "section": "Recent Additions",
    "text": "Recent Additions\n\nRandom Forest Analysis: Enhanced with interactive visualizations and decision boundary plots (View Analysis).\nCrewAI Swarm System: Full-featured, multi-swarm agent orchestration, ML/research/business/reporting specializations (README)\nTerminal Coding Agents: Terminal-based agent system, built-in ‚Äúbuild‚Äù and ‚Äúplan‚Äù agents with live installation quick start (README)\nJupyter Notebooks: ML and pandas essentials, plus scikit-learn examples (notebooks/)\n.gitignore Improvements: Now excludes macOS ._*, .DS_Store, and other platform/editor artifacts (.gitignore)\nChangelog: Track recent changes in CHANGELOG.md"
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "GitHub-Based Portfolio & Gallery of Jack J. Burleson",
    "section": "Skills",
    "text": "Skills\n\nProgramming: Python (advanced), R, JavaScript, bash, Make\nData Analysis: Pandas, NumPy, scikit-learn, seaborn, matplotlib\nVisualization: matplotlib, seaborn, Quarto, Jupyter\nMachine Learning & AI: scikit-learn, CrewAI, agent-based simulation, feature engineering\nDocumentation: Quarto, Markdown, Jupyter Notebooks\nCI/CD & Tooling: pre-commit, Black, GitHub Actions, Poetry, .gitignore hygiene\nOther: Git, SQL, Docker, technical writing, code review"
  },
  {
    "objectID": "index.html#publications-presentations",
    "href": "index.html#publications-presentations",
    "title": "GitHub-Based Portfolio & Gallery of Jack J. Burleson",
    "section": "Publications & Presentations",
    "text": "Publications & Presentations\n\nRandom Forest Essentials ‚Äì Quarto doc, 2024. (Link)\nTalking to Agents: architecting Multi-Agent Systems ‚Äì Internal seminar, 2024.\n(More manuscripts and presentations to be added.)\nFor a list of all talks and publications, see docs/publications.md (Coming soon)"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "GitHub-Based Portfolio & Gallery of Jack J. Burleson",
    "section": "Contact",
    "text": "Contact\n\nEmail: jackburleson.dev@gmail.com\nLinkedIn: linkedin.com/in/jack-j-burleson\nWebsite: jackburleson.dev"
  },
  {
    "objectID": "index.html#socials",
    "href": "index.html#socials",
    "title": "GitHub-Based Portfolio & Gallery of Jack J. Burleson",
    "section": "Socials",
    "text": "Socials\n\nGitHub\nTwitter/X\nLinkedIn\nHuggingFace"
  },
  {
    "objectID": "index.html#further-reading",
    "href": "index.html#further-reading",
    "title": "GitHub-Based Portfolio & Gallery of Jack J. Burleson",
    "section": "Further Reading",
    "text": "Further Reading\n\nTerminal Agents Project README\nCrewAI Multi-Agent System README\nCHANGELOG.md\nSECURITY.md\nOfficial Python Template Repo\n\nRepository updated regularly. Check project directories for the latest code, docs, and research."
  },
  {
    "objectID": "SECURITY.html",
    "href": "SECURITY.html",
    "title": "Security Policy",
    "section": "",
    "text": "Welcome to this repository! Security is a top priority for all contributors, maintainers, and users. This SECURITY.md document describes security practices, responsible disclosure, and guidance for keeping the repository safe for everyone.\n\n\n\n\n\n\nVersion\nSupported\nMaintenance\n\n\n\n\nLatest\n:white_check_mark:\n:white_check_mark:\n\n\nPrevious\n:white_check_mark:\n:x:\n\n\n\nOnly the latest version is actively maintained and receives security updates.\n\n\n\n\nIf you discover a security vulnerability, please follow these steps:\n\nPrivately disclose the issue.\nEmail us at security@opencode.ai or contact a maintainer directly. Avoid creating public GitHub issues for vulnerabilities.\nInclude details:\n\nAffected files and versions\nSteps to reproduce\nAny relevant logs or screenshots\nYour suggested mitigation or patch (if available)\n\nWe will:\n\nRespond within 2 business days\nTriage and validate the report\nRelease a fix as quickly as possible\nCredit you (with permission) after a public disclosure\n\n\n\n\n\n\n\n\n\nKeep up-to-date: Always use the latest release from GitHub or official sites.\nVerify downloads: Check for sha256 signatures when available.\nNever share sensitive credentials in issues, PRs, or chat logs.\nReview third-party dependencies: This repo uses several. Monitor for CVEs and update dependencies regularly.\n\n\n\n\n\nSign your commits (Guide):\n\ngit config --global commit.gpgsign true\n\nDo not include secrets (API keys, tokens) in code, configs, or docs.\nAdd unit/integration tests for all security fixes.\nRun static analysis and linters before submitting code.\nLockdown GitHub Actions: Reference only trusted actions, pin to a specific commit SHA.\nFollow the Principle of Least Privilege in all scripts and integrations.\nKeep dependencies updated; submit a PR if you spot an outdated or vulnerable one.\n\n\n\n\n\n\nThis repository makes use of open-source tools and packages (see requirements.txt and package.json if available).\n\nReview the dependencies for newly announced vulnerabilities\nUse pip list --outdated and npm audit (where applicable)\nManaged dependencies are periodically updated by maintainers\n\n\n\n\n\n\nDefault settings avoid privilege escalation and limit shell command execution.\nDo not run scripts with elevated privileges (sudo/root) unless explicitly required.\nKill unnecessary or idle processes and remove temporary files after execution (see scripts/free_ram.sh for an example).\nMonitor for unauthorized access or unexpected network traffic during use.\nUse secure communication methods (e.g., SSH, HTTPS).\n\n\n\n\n\nWe adhere to responsible disclosure principles.\nAll security concerns will be kept confidential until a patch is available and users have reasonable time to upgrade.\n\n\n\n\n\nEmail: security@opencode.ai\nDiscord: https://opencode.ai/discord\nOr contact a maintainer privately\n\n\nThank you for helping keep the project and its users secure!"
  },
  {
    "objectID": "SECURITY.html#supported-versions",
    "href": "SECURITY.html#supported-versions",
    "title": "Security Policy",
    "section": "",
    "text": "Version\nSupported\nMaintenance\n\n\n\n\nLatest\n:white_check_mark:\n:white_check_mark:\n\n\nPrevious\n:white_check_mark:\n:x:\n\n\n\nOnly the latest version is actively maintained and receives security updates."
  },
  {
    "objectID": "SECURITY.html#reporting-a-vulnerability",
    "href": "SECURITY.html#reporting-a-vulnerability",
    "title": "Security Policy",
    "section": "",
    "text": "If you discover a security vulnerability, please follow these steps:\n\nPrivately disclose the issue.\nEmail us at security@opencode.ai or contact a maintainer directly. Avoid creating public GitHub issues for vulnerabilities.\nInclude details:\n\nAffected files and versions\nSteps to reproduce\nAny relevant logs or screenshots\nYour suggested mitigation or patch (if available)\n\nWe will:\n\nRespond within 2 business days\nTriage and validate the report\nRelease a fix as quickly as possible\nCredit you (with permission) after a public disclosure"
  },
  {
    "objectID": "SECURITY.html#security-best-practices",
    "href": "SECURITY.html#security-best-practices",
    "title": "Security Policy",
    "section": "",
    "text": "Keep up-to-date: Always use the latest release from GitHub or official sites.\nVerify downloads: Check for sha256 signatures when available.\nNever share sensitive credentials in issues, PRs, or chat logs.\nReview third-party dependencies: This repo uses several. Monitor for CVEs and update dependencies regularly.\n\n\n\n\n\nSign your commits (Guide):\n\ngit config --global commit.gpgsign true\n\nDo not include secrets (API keys, tokens) in code, configs, or docs.\nAdd unit/integration tests for all security fixes.\nRun static analysis and linters before submitting code.\nLockdown GitHub Actions: Reference only trusted actions, pin to a specific commit SHA.\nFollow the Principle of Least Privilege in all scripts and integrations.\nKeep dependencies updated; submit a PR if you spot an outdated or vulnerable one."
  },
  {
    "objectID": "SECURITY.html#third-party-dependencies",
    "href": "SECURITY.html#third-party-dependencies",
    "title": "Security Policy",
    "section": "",
    "text": "This repository makes use of open-source tools and packages (see requirements.txt and package.json if available).\n\nReview the dependencies for newly announced vulnerabilities\nUse pip list --outdated and npm audit (where applicable)\nManaged dependencies are periodically updated by maintainers"
  },
  {
    "objectID": "SECURITY.html#build-runtime-security",
    "href": "SECURITY.html#build-runtime-security",
    "title": "Security Policy",
    "section": "",
    "text": "Default settings avoid privilege escalation and limit shell command execution.\nDo not run scripts with elevated privileges (sudo/root) unless explicitly required.\nKill unnecessary or idle processes and remove temporary files after execution (see scripts/free_ram.sh for an example).\nMonitor for unauthorized access or unexpected network traffic during use.\nUse secure communication methods (e.g., SSH, HTTPS)."
  },
  {
    "objectID": "SECURITY.html#responsible-disclosure",
    "href": "SECURITY.html#responsible-disclosure",
    "title": "Security Policy",
    "section": "",
    "text": "We adhere to responsible disclosure principles.\nAll security concerns will be kept confidential until a patch is available and users have reasonable time to upgrade."
  },
  {
    "objectID": "SECURITY.html#security-contact",
    "href": "SECURITY.html#security-contact",
    "title": "Security Policy",
    "section": "",
    "text": "Email: security@opencode.ai\nDiscord: https://opencode.ai/discord\nOr contact a maintainer privately\n\n\nThank you for helping keep the project and its users secure!"
  },
  {
    "objectID": "projects/terminal_agents/README.html",
    "href": "projects/terminal_agents/README.html",
    "title": "JJB Gallery",
    "section": "",
    "text": "The AI coding agent built for the terminal.\n  \n\n\n\nOpenCode Terminal UI\n\n\n\n\nInstallation\n# YOLO\ncurl -fsSL https://opencode.ai/install | bash\n\n# Package managers\nnpm i -g opencode-ai@latest        # or bun/pnpm/yarn\nscoop bucket add extras; scoop install extras/opencode  # Windows\nchoco install opencode             # Windows\nbrew install opencode              # macOS and Linux\nparu -S opencode-bin               # Arch Linux\nmise use --pin -g ubi:sst/opencode # Any OS\nnix run nixpkgs#opencode           # or github:sst/opencode for latest dev branch\n\n[!TIP] Remove versions older than 0.1.x before installing.\n\n\nInstallation Directory\nThe install script respects the following priority order for the installation path:\n\n$OPENCODE_INSTALL_DIR - Custom installation directory\n$XDG_BIN_DIR - XDG Base Directory Specification compliant path\n$HOME/bin - Standard user binary directory (if exists or can be created)\n$HOME/.opencode/bin - Default fallback\n\n# Examples\nOPENCODE_INSTALL_DIR=/usr/local/bin curl -fsSL https://opencode.ai/install | bash\nXDG_BIN_DIR=$HOME/.local/bin curl -fsSL https://opencode.ai/install | bash\n\n\n\nAgents\nOpenCode includes two built-in agents you can switch between, you can switch between these using the Tab key.\n\nbuild - Default, full access agent for development work\nplan - Read-only agent for analysis and code exploration\n\nDenies file edits by default\nAsks permission before running bash commands\nIdeal for exploring unfamiliar codebases or planning changes\n\n\nAlso, included is a general subagent for complex searches and multi-step tasks. This is used internally and can be invoked using @general in messages.\nLearn more about agents.\n\n\nDocumentation\nFor more info on how to configure OpenCode head over to our docs.\n\n\nContributing\nIf you‚Äôre interested in contributing to OpenCode, please read our contributing docs before submitting a pull request.\n\n\nBuilding on OpenCode\nIf you are working on a project that‚Äôs related to OpenCode and is using ‚Äúopencode‚Äù as a part of its name; for example, ‚Äúopencode-dashboard‚Äù or ‚Äúopencode-mobile‚Äù, please add a note to your README to clarify that it is not built by the OpenCode team and is not affiliated with us in anyway.\n\n\nFAQ\n\nHow is this different than Claude Code?\nIt‚Äôs very similar to Claude Code in terms of capability. Here are the key differences:\n\n100% open source\nNot coupled to any provider. Although we recommend the models we provide through OpenCode Zen; OpenCode can be used with Claude, OpenAI, Google or even local models. As models evolve the gaps between them will close and pricing will drop so being provider-agnostic is important.\nOut of the box LSP support\nA focus on TUI. OpenCode is built by neovim users and the creators of terminal.shop; we are going to push the limits of what‚Äôs possible in the terminal.\nA client/server architecture. This for example can allow OpenCode to run on your computer, while you can drive it remotely from a mobile app. Meaning that the TUI frontend is just one of the possible clients.\n\n\n\nWhat‚Äôs the other repo?\nThe other confusingly named repo has no relation to this one. You can read the story behind it here.\n\nJoin our community Discord | X.com"
  }
]