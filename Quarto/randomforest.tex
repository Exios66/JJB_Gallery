% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Random Forests in Practice},
  pdfauthor={Data Science Lab},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Random Forests in Practice}
\author{Data Science Lab}
\date{2025-11-21}
\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{3}
\tableofcontents
}

\subsection{Overview}\label{overview}

Random forests are ensemble models that aggregate many decision trees to
reduce variance and improve generalization.\footnote{Introduced the
  random forest algorithm with theoretical justification and empirical
  benchmarks.} This document walks through training and interpreting a
random forest classifier in Python, with a mix of narrative, math, and
visuals.

\subsection{Mathematical Model}\label{mathematical-model}

Each tree (T\_b) is trained on a bootstrap sample (\mathcal{D}\_b) and a
random subset of features. The forest prediction for a classification
task with (B) trees is the majority vote:

{[} \hat{y} =
\mathrm{mode}\left(\{T\_b(\mathbf{x})\}\_\{b=1\}\^{}\{B\}\right) {]}

For regression, the trees are averaged:

{[} \hat{y} = \frac{1}{B}\sum\_\{b=1\}\^{}\{B\} T\_b(\mathbf{x}) {]}

The randomization across bootstrapped data and feature subsampling
drives decorrelation between trees, delivering lower variance than
single-tree models.

\subsection{Environment Setup}\label{environment-setup}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ importlib}
\ImportTok{import}\NormalTok{ subprocess}
\ImportTok{import}\NormalTok{ sys}

\KeywordTok{def}\NormalTok{ ensure(package):}
    \ControlFlowTok{try}\NormalTok{:}
\NormalTok{        importlib.import\_module(package)}
    \ControlFlowTok{except} \PreprocessorTok{ImportError}\NormalTok{:}
\NormalTok{        subprocess.check\_call([sys.executable, }\StringTok{"{-}m"}\NormalTok{, }\StringTok{"pip"}\NormalTok{, }\StringTok{"install"}\NormalTok{, package])}

\ControlFlowTok{for}\NormalTok{ pkg }\KeywordTok{in}\NormalTok{ (}\StringTok{"numpy"}\NormalTok{, }\StringTok{"pandas"}\NormalTok{, }\StringTok{"seaborn"}\NormalTok{, }\StringTok{"matplotlib"}\NormalTok{, }\StringTok{"scikit{-}learn"}\NormalTok{):}
\NormalTok{    ensure(pkg)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Requirement already satisfied: scikit-learn in /Users/luciusjmorningstar/Desktop/GIT-REPOSITORY/JJB_Gallery/.venv/lib/python3.13/site-packages (1.7.2)
Requirement already satisfied: numpy>=1.22.0 in /Users/luciusjmorningstar/Desktop/GIT-REPOSITORY/JJB_Gallery/.venv/lib/python3.13/site-packages (from scikit-learn) (2.3.5)
Requirement already satisfied: scipy>=1.8.0 in /Users/luciusjmorningstar/Desktop/GIT-REPOSITORY/JJB_Gallery/.venv/lib/python3.13/site-packages (from scikit-learn) (1.16.3)
Requirement already satisfied: joblib>=1.2.0 in /Users/luciusjmorningstar/Desktop/GIT-REPOSITORY/JJB_Gallery/.venv/lib/python3.13/site-packages (from scikit-learn) (1.5.2)
Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/luciusjmorningstar/Desktop/GIT-REPOSITORY/JJB_Gallery/.venv/lib/python3.13/site-packages (from scikit-learn) (3.6.0)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\ImportTok{from}\NormalTok{ sklearn.datasets }\ImportTok{import}\NormalTok{ load\_breast\_cancer}
\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ train\_test\_split, cross\_val\_score}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ RandomForestClassifier}
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ RocCurveDisplay, ConfusionMatrixDisplay, classification\_report}

\NormalTok{sns.set\_theme(style}\OperatorTok{=}\StringTok{"whitegrid"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Data Loading and
Preparation}\label{data-loading-and-preparation}

We will use the Breast Cancer Wisconsin dataset bundled with
scikit-learn, which contains 30 features computed from digitized fine
needle aspirate images.\footnote{Official description of the dataset,
  feature definitions, and usage considerations.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ load\_breast\_cancer(as\_frame}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{df }\OperatorTok{=}\NormalTok{ dataset.frame}
\NormalTok{df.head()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llllllllllllllllllllll@{}}
\toprule\noalign{}
& mean radius & mean texture & mean perimeter & mean area & mean
smoothness & mean compactness & mean concavity & mean concave points &
mean symmetry & mean fractal dimension & ... & worst texture & worst
perimeter & worst area & worst smoothness & worst compactness & worst
concavity & worst concave points & worst symmetry & worst fractal
dimension & target \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 17.99 & 10.38 & 122.80 & 1001.0 & 0.11840 & 0.27760 & 0.3001 &
0.14710 & 0.2419 & 0.07871 & ... & 17.33 & 184.60 & 2019.0 & 0.1622 &
0.6656 & 0.7119 & 0.2654 & 0.4601 & 0.11890 & 0 \\
1 & 20.57 & 17.77 & 132.90 & 1326.0 & 0.08474 & 0.07864 & 0.0869 &
0.07017 & 0.1812 & 0.05667 & ... & 23.41 & 158.80 & 1956.0 & 0.1238 &
0.1866 & 0.2416 & 0.1860 & 0.2750 & 0.08902 & 0 \\
2 & 19.69 & 21.25 & 130.00 & 1203.0 & 0.10960 & 0.15990 & 0.1974 &
0.12790 & 0.2069 & 0.05999 & ... & 25.53 & 152.50 & 1709.0 & 0.1444 &
0.4245 & 0.4504 & 0.2430 & 0.3613 & 0.08758 & 0 \\
3 & 11.42 & 20.38 & 77.58 & 386.1 & 0.14250 & 0.28390 & 0.2414 & 0.10520
& 0.2597 & 0.09744 & ... & 26.50 & 98.87 & 567.7 & 0.2098 & 0.8663 &
0.6869 & 0.2575 & 0.6638 & 0.17300 & 0 \\
4 & 20.29 & 14.34 & 135.10 & 1297.0 & 0.10030 & 0.13280 & 0.1980 &
0.10430 & 0.1809 & 0.05883 & ... & 16.67 & 152.20 & 1575.0 & 0.1374 &
0.2050 & 0.4000 & 0.1625 & 0.2364 & 0.07678 & 0 \\
\end{longtable}

Split the data into training and testing sets (stratified to maintain
label balance).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X\_train, X\_test, y\_train, y\_test }\OperatorTok{=}\NormalTok{ train\_test\_split(}
\NormalTok{    df.drop(columns}\OperatorTok{=}\StringTok{"target"}\NormalTok{),}
\NormalTok{    df[}\StringTok{"target"}\NormalTok{],}
\NormalTok{    test\_size}\OperatorTok{=}\FloatTok{0.25}\NormalTok{,}
\NormalTok{    random\_state}\OperatorTok{=}\DecValTok{42}\NormalTok{,}
\NormalTok{    stratify}\OperatorTok{=}\NormalTok{df[}\StringTok{"target"}\NormalTok{]}
\NormalTok{)}

\NormalTok{X\_train.shape, X\_test.shape}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
((426, 30), (143, 30))
\end{verbatim}

\subsection{Model Training}\label{model-training}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rf }\OperatorTok{=}\NormalTok{ RandomForestClassifier(}
\NormalTok{    n\_estimators}\OperatorTok{=}\DecValTok{400}\NormalTok{,}
\NormalTok{    max\_features}\OperatorTok{=}\StringTok{"sqrt"}\NormalTok{,}
\NormalTok{    min\_samples\_leaf}\OperatorTok{=}\DecValTok{2}\NormalTok{,}
\NormalTok{    random\_state}\OperatorTok{=}\DecValTok{42}\NormalTok{,}
\NormalTok{    n\_jobs}\OperatorTok{={-}}\DecValTok{1}
\NormalTok{)}
\NormalTok{rf.fit(X\_train, y\_train)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\emph{} & n\_estimators~ & 400 \\
\emph{} & criterion~ & \textquotesingle gini\textquotesingle{} \\
\emph{} & max\_depth~ & None \\
\emph{} & min\_samples\_split~ & 2 \\
\emph{} & min\_samples\_leaf~ & 2 \\
\emph{} & min\_weight\_fraction\_leaf~ & 0.0 \\
\emph{} & max\_features~ & \textquotesingle sqrt\textquotesingle{} \\
\emph{} & max\_leaf\_nodes~ & None \\
\emph{} & min\_impurity\_decrease~ & 0.0 \\
\emph{} & bootstrap~ & True \\
\emph{} & oob\_score~ & False \\
\emph{} & n\_jobs~ & -1 \\
\emph{} & random\_state~ & 42 \\
\emph{} & verbose~ & 0 \\
\emph{} & warm\_start~ & False \\
\emph{} & class\_weight~ & None \\
\emph{} & ccp\_alpha~ & 0.0 \\
\emph{} & max\_samples~ & None \\
\emph{} & monotonic\_cst~ & None \\
\end{longtable}

Evaluate cross-validated training performance to estimate generalization
ability.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cv\_scores }\OperatorTok{=}\NormalTok{ cross\_val\_score(rf, X\_train, y\_train, cv}\OperatorTok{=}\DecValTok{5}\NormalTok{)}
\NormalTok{cv\_scores.mean(), cv\_scores.std()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(np.float64(0.9600547195622434), np.float64(0.020556647327639923))
\end{verbatim}

\subsection{Diagnostics}\label{diagnostics}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y\_pred }\OperatorTok{=}\NormalTok{ rf.predict(X\_test)}
\BuiltInTok{print}\NormalTok{(classification\_report(y\_test, y\_pred, target\_names}\OperatorTok{=}\NormalTok{dataset.target\_names))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
              precision    recall  f1-score   support

   malignant       0.96      0.92      0.94        53
      benign       0.96      0.98      0.97        90

    accuracy                           0.96       143
   macro avg       0.96      0.95      0.95       143
weighted avg       0.96      0.96      0.96       143
\end{verbatim}

\subsubsection{Receiver Operating
Characteristic}\label{receiver-operating-characteristic}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{4}\NormalTok{))}
\NormalTok{RocCurveDisplay.from\_estimator(rf, X\_test, y\_test, ax}\OperatorTok{=}\NormalTok{ax)}
\NormalTok{ax.set\_title(}\StringTok{"Random Forest ROC Curve"}\NormalTok{)}
\NormalTok{plt.tight\_layout()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{randomforest_files/figure-pdf/cell-9-output-1.pdf}}

\subsubsection{Confusion Matrix}\label{confusion-matrix}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{))}
\NormalTok{ConfusionMatrixDisplay.from\_estimator(rf, X\_test, y\_test, display\_labels}\OperatorTok{=}\NormalTok{dataset.target\_names, ax}\OperatorTok{=}\NormalTok{ax, cmap}\OperatorTok{=}\StringTok{"Blues"}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{"Confusion Matrix"}\NormalTok{)}
\NormalTok{plt.tight\_layout()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{randomforest_files/figure-pdf/cell-10-output-1.pdf}}

\subsection{Feature Importance
Visualization}\label{feature-importance-visualization}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{importances }\OperatorTok{=}\NormalTok{ pd.Series(rf.feature\_importances\_, index}\OperatorTok{=}\NormalTok{df.columns[:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]).sort\_values(ascending}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{top\_features }\OperatorTok{=}\NormalTok{ importances.head(}\DecValTok{15}\NormalTok{)}

\ControlFlowTok{try}\NormalTok{:}
\NormalTok{    sns}
\ControlFlowTok{except} \PreprocessorTok{NameError}\NormalTok{:}
    \ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\NormalTok{    sns.set\_theme(style}\OperatorTok{=}\StringTok{"whitegrid"}\NormalTok{)}

\NormalTok{plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{sns.barplot(x}\OperatorTok{=}\NormalTok{top\_features.values, y}\OperatorTok{=}\NormalTok{top\_features.index, palette}\OperatorTok{=}\StringTok{"viridis"}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"Top 15 Feature Importances"}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{"Gini Importance"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Feature"}\NormalTok{)}
\NormalTok{plt.tight\_layout()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{randomforest_files/figure-pdf/cell-11-output-1.pdf}}

\subsection{Hyperparameter
Considerations}\label{hyperparameter-considerations}

\begin{itemize}
\tightlist
\item
  \texttt{n\_estimators}: Increasing trees generally improves stability
  until diminishing returns set in.
\item
  \texttt{max\_depth} or \texttt{min\_samples\_leaf}: Control tree
  complexity, mitigating overfitting.
\item
  \texttt{max\_features}: Governs the degree of feature randomness;
  \texttt{sqrt} is typical for classification.
\item
  \texttt{class\_weight}: Useful for imbalanced datasets to penalize
  misclassification of minority classes.
\end{itemize}

Grid search or Bayesian optimization can systematically explore these
settings.\footnote{Demonstrated the efficiency gains of random search
  over grid search for hyperparameter tuning.}

\subsection{Practical Tips}\label{practical-tips}

\begin{itemize}
\tightlist
\item
  \textbf{Feature scaling}: Not required because trees are invariant to
  monotonic transformations.
\item
  \textbf{Missing values}: scikit-learn's implementation does not handle
  NaNs; impute beforehand.
\item
  \textbf{Interpretability}: Use SHAP values or permutation importance
  for richer explanations.
\item
  \textbf{Out-of-bag (OOB) estimates}: Enable \texttt{oob\_score=True}
  to get a built-in validation metric without a separate hold-out set.
\end{itemize}

\subsection{References}\label{references}

\begin{itemize}
\tightlist
\item
  Breiman, L. (2001). Random forests. \emph{Machine Learning}, 45(1),
  5--32. \url{https://doi.org/10.1023/A:1010933404324}\footnote{\^{}breiman2001}
\item
  scikit-learn Breast Cancer Dataset docs.
  \url{https://scikit-learn.org/stable/datasets/toy_dataset.html}\footnote{\^{}sklearn\_breast}
\item
  Bergstra, J., \& Bengio, Y. (2012). Random search for hyper-parameter
  optimization. \emph{Journal of Machine Learning Research}, 13,
  281--305.
  \url{https://jmlr.org/papers/v13/bergstra12a.html}\footnote{\^{}bergstra2012}
\end{itemize}




\end{document}
